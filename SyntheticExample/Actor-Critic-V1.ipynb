{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from keras.layers import LSTM, Dense, Activation, Input, Lambda, Concatenate\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1º versión funcionando (A-C Experience Replay 1-batchsize) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make synthetic problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_sample = 16\n",
    "dims = 16\n",
    "n_samples = 500\n",
    "documents, summaries = [], []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    document, summary = [], []\n",
    "    for j in range(len_sample):\n",
    "        rnd = random.randint(0, 1)\n",
    "        if rnd == 0:\n",
    "            s = np.random.normal(loc = 0., scale = 0.2, size = (dims,))\n",
    "            document.append(s)\n",
    "        else:\n",
    "            s = np.random.normal(loc = 1, scale = 0.1, size = (dims,))\n",
    "            document.append(s)\n",
    "            summary.append(s)\n",
    "            \n",
    "    documents.append(document)\n",
    "    summaries.append(summary)\n",
    "    \n",
    "documents = np.array(documents)\n",
    "summaries = np.array(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor Critic Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent:\n",
    "    \n",
    "    def __init__(self, doc_state_dim, summ_state_dim, action_dim = 2):\n",
    "        \n",
    "        self.doc_state_dim = doc_state_dim\n",
    "        self.summ_state_dim = summ_state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.discount_factor = 0.99\n",
    "        self.actor_lr = 0.001\n",
    "        self.critic_lr = 0.005\n",
    "  \n",
    "        self.exploration_max = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = 0.9995\n",
    "        self.exploration_rate = self.exploration_max\n",
    "        \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        \n",
    "        self.lstm_dims = 16\n",
    "        self.reader, self.actor, self.critic = self.build_models()\n",
    "\n",
    "        \n",
    "    def build_models(self):\n",
    "        \n",
    "        doc_state = Input(shape=(None, dims))\n",
    "        summ_state = Input(shape=(None, dims))\n",
    "        state_h = Input(shape=(self.lstm_dims,))\n",
    "        state_c = Input(shape=(self.lstm_dims,))\n",
    "        \n",
    "        lstm = LSTM(self.lstm_dims, activation = \"tanh\", name = \"lstm_1\", return_sequences=False, return_state=True)\n",
    "        \n",
    "        o1, lstm_state_h, lstm_state_c  = lstm(doc_state, initial_state = [state_h, state_c])\n",
    "        o2, _, _  = lstm(summ_state, initial_state = [state_h, state_c])\n",
    "    \n",
    "        diff = Lambda(lambda x: K.abs(x[0] - x[1]))([o1, o2])\n",
    "        diff = Concatenate()([o1, o2, diff])\n",
    "        \n",
    "        # Shared actor and critic #\n",
    "        actor_output = Dense(self.action_dim, activation = \"softmax\")(diff)\n",
    "        critic_output = Dense(1, activation=\"linear\")(diff)\n",
    "        \n",
    "        reader = Model(inputs = [doc_state, state_h, state_c], outputs = [lstm_state_h, lstm_state_c])\n",
    "        actor = Model(inputs = [doc_state, summ_state, state_h, state_c], outputs = actor_output)\n",
    "        critic = Model(inputs = [doc_state, summ_state, state_h, state_c], outputs = critic_output)\n",
    "\n",
    "        actor.compile(loss='categorical_crossentropy', optimizer=Adam(lr=self.actor_lr))\n",
    "        critic.compile(loss=\"mse\", optimizer=Adam(lr=self.critic_lr))\n",
    "                       \n",
    "        return reader, actor, critic\n",
    "                        \n",
    "\n",
    "    # using the output of policy network, pick action stochastically\n",
    "    def get_action(self, doc_state, summ_state, state_h, state_c):\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            return random.randrange(self.action_dim)\n",
    "        policy = self.actor.predict([doc_state, summ_state, state_h, state_c])[0]\n",
    "        r = np.random.choice(self.action_dim, 1, p=policy)[0]\n",
    "        return r\n",
    "\n",
    "                       \n",
    "    def remember(self, doc_state, summ_state, state_h, state_c, \n",
    "                 action, reward, next_doc_state, next_summ_state, \n",
    "                 done):\n",
    "        \n",
    "        self.memory.append((doc_state, summ_state, state_h, state_c,\n",
    "                            action, reward, next_doc_state, next_summ_state,\n",
    "                            done)) \n",
    "     \n",
    "    # update policy network every episode (Memory Replay & Batchified)\n",
    "    def train_model(self):\n",
    "        if len(self.memory) < MEMORY_SIZE:\n",
    "            return\n",
    "        \n",
    "        target = np.zeros(1,)\n",
    "        advantage = np.zeros((self.action_dim,))\n",
    "        \n",
    "        rnd_idx = random.randint(0, len(self.memory)-1)\n",
    "        (doc_state, summ_state, state_h, state_c,\n",
    "        action, reward, next_doc_state, next_summ_state,\n",
    "        done)= self.memory[rnd_idx]\n",
    "        summ_state = np.array([summ_state])\n",
    "        next_summ_state = np.array([next_summ_state])\n",
    "\n",
    "        values = self.critic.predict([doc_state, summ_state, state_h, state_c])[0]     \n",
    "        next_values = self.critic.predict([next_doc_state, next_summ_state, state_h, state_c])[0]\n",
    "              \n",
    "        \n",
    "        for i in range(1):\n",
    "            if done:\n",
    "                advantage[action] = max(min(1., reward - values), 0.)\n",
    "                target[0] = reward\n",
    "            else:\n",
    "                # Explicacion: https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html#actor-critic\n",
    "                advantage[action] = max(min(1., reward + self.discount_factor * (next_values) - values), 0.)\n",
    "                target[0] = reward + self.discount_factor * next_values\n",
    "                \n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        self.exploration_rate = max(self.exploration_min, self.exploration_rate)\n",
    "        \n",
    "        advantage = np.array([advantage])\n",
    "        target = np.array([target])\n",
    "\n",
    "        self.actor.fit([doc_state, summ_state, state_h, state_c], advantage, epochs=1, batch_size = 1, verbose=0)\n",
    "        self.critic.fit([doc_state, summ_state, state_h, state_c], target, epochs=1, batch_size = 1, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(gen_summary, summary):\n",
    "    acum_sims = 0.\n",
    "    for i in range(len(gen_summary)):\n",
    "        found = False\n",
    "        for j in range(len(summary)):\n",
    "            if (gen_summary[i] == summary[j]).all() == True:\n",
    "                found = True\n",
    "                break\n",
    "        if found == True:\n",
    "            acum_sims += 1\n",
    "        else:\n",
    "            acum_sims -= 1\n",
    "    return acum_sims #/ max(len(gen_summary), len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg reward on episode: 0 -> 1.912\n",
      "Avg reward on episode: 1 -> 5.740\n",
      "Avg reward on episode: 2 -> 5.956\n",
      "Avg reward on episode: 3 -> 6.568\n",
      "Avg reward on episode: 4 -> 6.960\n",
      "Avg reward on episode: 5 -> 6.652\n",
      "Avg reward on episode: 6 -> 6.766\n",
      "Avg reward on episode: 7 -> 6.972\n",
      "Avg reward on episode: 8 -> 6.910\n",
      "Avg reward on episode: 9 -> 7.260\n"
     ]
    }
   ],
   "source": [
    "EPISODES = 10\n",
    "N_SAMPLES = n_samples\n",
    "BATCH_SIZE = 1\n",
    "MEMORY_SIZE = 1024\n",
    "\n",
    "# get size of state and action from environment\n",
    "doc_state_dim = summ_state_dim = dims\n",
    "action_dim = 2\n",
    "\n",
    "# make A2C agent\n",
    "agent = A2CAgent(doc_state_dim, summ_state_dim, action_dim)\n",
    "\n",
    "\n",
    "for e in range(EPISODES):\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(N_SAMPLES):\n",
    "        document = documents[i]\n",
    "        summary = summaries[i]\n",
    "        \n",
    "        # Leer doc por completo #\n",
    "        init_c_state = np.zeros((1, 16)) + 1e-16\n",
    "        init_h_state = np.zeros((1, 16)) + 1e-16\n",
    "        lstm_h_state, lstm_c_state = agent.reader.predict([np.array([document]), init_h_state, init_c_state])\n",
    "\n",
    "        summ_state = [np.zeros(summ_state_dim) + 1e-16]\n",
    "        actions = []\n",
    "\n",
    "        for j in range(len(document)):\n",
    "\n",
    "            doc_state = np.array([document[0 : j + 1]])\n",
    "            next_doc_state = np.array([document[0 : j + 2]])\n",
    "            next_summ_state = summ_state[:]\n",
    "\n",
    "            action = agent.get_action(doc_state, np.array([summ_state]), lstm_h_state, lstm_c_state)\n",
    "            \n",
    "            if action == 1:\n",
    "                next_summ_state.append(document[j].tolist())\n",
    "                actions.append(1)\n",
    "            else:\n",
    "                actions.append(0)\n",
    "\n",
    "            reward = get_reward(np.array(next_summ_state[1:]), summary)\n",
    "\n",
    "            if j < len(document) - 1:\n",
    "                done = False\n",
    "            else:\n",
    "                done = True\n",
    "\n",
    "            agent.remember(doc_state, summ_state, lstm_h_state, lstm_c_state, \n",
    "                           action, reward, next_doc_state, next_summ_state, done)\n",
    "\n",
    "            agent.train_model()\n",
    "\n",
    "            summ_state = next_summ_state\n",
    "        scores.append(reward)\n",
    "        #print(\"Sample: %d, Actions: %s, Reward: %.4f\" % (i, str(actions), reward))\n",
    "    print(\"Avg reward on episode: %d -> %.3f\" % (e, np.array(scores).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = documents[4]\n",
    "summary = summaries[4]\n",
    "\n",
    "# Leer doc por completo #\n",
    "init_c_state = np.zeros((1, 16)) + 1e-16\n",
    "init_h_state = np.zeros((1, 16)) + 1e-16\n",
    "lstm_h_state, lstm_c_state = agent.reader.predict([np.array([document]), init_h_state, init_c_state])\n",
    "\n",
    "summ_state = [np.zeros(summ_state_dim) + 1e-16]\n",
    "actions = []\n",
    "\n",
    "for j in range(len(document)):\n",
    "\n",
    "    doc_state = np.array([document[0 : j + 1]])\n",
    "    next_doc_state = np.array([document[0 : j + 2]])\n",
    "    next_summ_state = summ_state[:]\n",
    "\n",
    "    action = agent.get_action(doc_state, np.array([summ_state]), lstm_h_state, lstm_c_state)\n",
    "\n",
    "    if action == 1:\n",
    "        next_summ_state.append(document[j].tolist())\n",
    "        actions.append(1)\n",
    "    else:\n",
    "        actions.append(0)\n",
    "\n",
    "    reward = get_reward(np.array(next_summ_state[1:]), summary)\n",
    "\n",
    "    if j < len(document) - 1:\n",
    "        done = False\n",
    "    else:\n",
    "        done = True\n",
    "    \n",
    "    summ_state = next_summ_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0926033 , -0.18200941, -0.13242487,  0.38948635,  0.06211748,\n",
       "        -0.45572787, -0.06779616, -0.21168341,  0.08835016, -0.23462003,\n",
       "        -0.1235629 , -0.05787389, -0.05209931,  0.08730754, -0.09298691,\n",
       "         0.27572553],\n",
       "       [ 0.32847872,  0.13556859, -0.15419601,  0.04420499,  0.0993616 ,\n",
       "         0.05395936, -0.04386647, -0.20157574, -0.0125658 , -0.14496135,\n",
       "         0.36515771, -0.1773753 ,  0.12540683,  0.21266409,  0.04859447,\n",
       "        -0.13192555],\n",
       "       [ 1.03383392,  0.97858038,  1.06356421,  0.94733702,  0.97512407,\n",
       "         0.97770182,  1.0662604 ,  1.13132905,  1.09529157,  0.94120605,\n",
       "         0.98215044,  0.85283281,  0.9238479 ,  1.15825165,  0.94103274,\n",
       "         0.97156026],\n",
       "       [ 0.98122508,  1.03821773,  0.9054462 ,  0.89277277,  0.96816157,\n",
       "         1.05397734,  1.18913346,  0.84027841,  1.08830363,  0.92971897,\n",
       "         1.10815857,  1.0450787 ,  1.12114833,  0.96708264,  0.96321032,\n",
       "         1.01580441],\n",
       "       [-0.19205855, -0.34768088, -0.14339264,  0.46294485, -0.05329752,\n",
       "         0.17363043,  0.05011497,  0.42610904, -0.36438864,  0.06114364,\n",
       "         0.03680114,  0.44175744, -0.14436598, -0.28095669,  0.14105926,\n",
       "         0.35689341],\n",
       "       [ 0.11783568, -0.2125292 , -0.16994415, -0.34936092,  0.08760413,\n",
       "        -0.07217688,  0.16348696,  0.15940057, -0.32808525, -0.08842189,\n",
       "        -0.15966546,  0.20327381,  0.08929371, -0.15179489,  0.44049604,\n",
       "        -0.01611463],\n",
       "       [ 0.93718471,  0.75003822,  0.94647728,  1.2178097 ,  0.95452597,\n",
       "         1.12183398,  0.80789445,  1.02298515,  1.11002949,  0.98113608,\n",
       "         0.97309826,  1.02898817,  1.07190151,  1.01635855,  0.92094405,\n",
       "         1.07269246],\n",
       "       [ 0.05263578,  0.08064233, -0.1756429 ,  0.00688841, -0.116036  ,\n",
       "        -0.08478571,  0.19744988, -0.07722877, -0.0257887 , -0.01827401,\n",
       "        -0.27929268, -0.13587794, -0.16342892, -0.19225701, -0.26667148,\n",
       "        -0.15621632],\n",
       "       [ 1.03581015,  1.14293062,  1.22965493,  0.956355  ,  0.87186639,\n",
       "         1.04662282,  0.98877225,  0.89037612,  1.03423343,  1.08351568,\n",
       "         1.02447254,  1.02152896,  0.98753297,  0.97952194,  1.08474729,\n",
       "         1.00228203],\n",
       "       [-0.08972419, -0.45002663,  0.06659415,  0.18878936,  0.0134434 ,\n",
       "        -0.02646647,  0.1491706 , -0.00144208,  0.30150494, -0.34947353,\n",
       "         0.07287443, -0.1391668 , -0.28181491,  0.29306014,  0.04736028,\n",
       "        -0.04167708],\n",
       "       [ 0.9986617 ,  1.0729275 ,  0.89186723,  0.8971852 ,  1.08159337,\n",
       "         0.78105736,  0.92045049,  0.97002241,  0.91287224,  1.0025745 ,\n",
       "         1.1054947 ,  1.06899402,  0.98847383,  0.99685189,  1.14613843,\n",
       "         1.0069557 ],\n",
       "       [-0.2935914 ,  0.25123821,  0.14633162,  0.3223732 ,  0.09951923,\n",
       "         0.06849627, -0.24062113, -0.2789993 , -0.47485799,  0.379204  ,\n",
       "         0.05939093, -0.19280925,  0.23774576, -0.05923753,  0.17332183,\n",
       "        -0.21512157],\n",
       "       [-0.24616798, -0.03184004,  0.08809212,  0.5592068 ,  0.37131855,\n",
       "        -0.08974549, -0.07266821, -0.01028829,  0.13574515, -0.12339635,\n",
       "        -0.03623963,  0.13102213,  0.36222416, -0.24491906, -0.38320232,\n",
       "         0.20077247],\n",
       "       [ 0.22038084, -0.22052221,  0.00668923,  0.07307354,  0.06530102,\n",
       "         0.2530922 , -0.09792055,  0.17043146, -0.0269768 , -0.12458332,\n",
       "        -0.10412113, -0.00427959,  0.12288961, -0.05955   ,  0.27233541,\n",
       "        -0.14157511],\n",
       "       [ 0.82752095,  1.03257376,  1.12704376,  0.87028255,  0.83393492,\n",
       "         0.93322103,  0.85547622,  1.02175917,  0.82847716,  1.11401238,\n",
       "         0.95314541,  1.09333589,  1.0750644 ,  1.14606288,  1.12141005,\n",
       "         1.19756883],\n",
       "       [ 0.21941304,  0.23814286,  0.00895572, -0.2451239 , -0.14278099,\n",
       "        -0.11465584, -0.03165142,  0.05787832,  0.07764423, -0.31675349,\n",
       "        -0.07681654, -0.10907542, -0.03397729, -0.22385969,  0.41125351,\n",
       "        -0.16768004]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.03383392, 0.97858038, 1.06356421, 0.94733702, 0.97512407,\n",
       "        0.97770182, 1.0662604 , 1.13132905, 1.09529157, 0.94120605,\n",
       "        0.98215044, 0.85283281, 0.9238479 , 1.15825165, 0.94103274,\n",
       "        0.97156026]),\n",
       " array([0.98122508, 1.03821773, 0.9054462 , 0.89277277, 0.96816157,\n",
       "        1.05397734, 1.18913346, 0.84027841, 1.08830363, 0.92971897,\n",
       "        1.10815857, 1.0450787 , 1.12114833, 0.96708264, 0.96321032,\n",
       "        1.01580441]),\n",
       " array([0.93718471, 0.75003822, 0.94647728, 1.2178097 , 0.95452597,\n",
       "        1.12183398, 0.80789445, 1.02298515, 1.11002949, 0.98113608,\n",
       "        0.97309826, 1.02898817, 1.07190151, 1.01635855, 0.92094405,\n",
       "        1.07269246]),\n",
       " array([1.03581015, 1.14293062, 1.22965493, 0.956355  , 0.87186639,\n",
       "        1.04662282, 0.98877225, 0.89037612, 1.03423343, 1.08351568,\n",
       "        1.02447254, 1.02152896, 0.98753297, 0.97952194, 1.08474729,\n",
       "        1.00228203]),\n",
       " array([0.9986617 , 1.0729275 , 0.89186723, 0.8971852 , 1.08159337,\n",
       "        0.78105736, 0.92045049, 0.97002241, 0.91287224, 1.0025745 ,\n",
       "        1.1054947 , 1.06899402, 0.98847383, 0.99685189, 1.14613843,\n",
       "        1.0069557 ]),\n",
       " array([0.82752095, 1.03257376, 1.12704376, 0.87028255, 0.83393492,\n",
       "        0.93322103, 0.85547622, 1.02175917, 0.82847716, 1.11401238,\n",
       "        0.95314541, 1.09333589, 1.0750644 , 1.14606288, 1.12141005,\n",
       "        1.19756883])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.03383392, 0.97858038, 1.06356421, 0.94733702, 0.97512407,\n",
       "        0.97770182, 1.0662604 , 1.13132905, 1.09529157, 0.94120605,\n",
       "        0.98215044, 0.85283281, 0.9238479 , 1.15825165, 0.94103274,\n",
       "        0.97156026],\n",
       "       [0.98122508, 1.03821773, 0.9054462 , 0.89277277, 0.96816157,\n",
       "        1.05397734, 1.18913346, 0.84027841, 1.08830363, 0.92971897,\n",
       "        1.10815857, 1.0450787 , 1.12114833, 0.96708264, 0.96321032,\n",
       "        1.01580441],\n",
       "       [0.93718471, 0.75003822, 0.94647728, 1.2178097 , 0.95452597,\n",
       "        1.12183398, 0.80789445, 1.02298515, 1.11002949, 0.98113608,\n",
       "        0.97309826, 1.02898817, 1.07190151, 1.01635855, 0.92094405,\n",
       "        1.07269246],\n",
       "       [1.03581015, 1.14293062, 1.22965493, 0.956355  , 0.87186639,\n",
       "        1.04662282, 0.98877225, 0.89037612, 1.03423343, 1.08351568,\n",
       "        1.02447254, 1.02152896, 0.98753297, 0.97952194, 1.08474729,\n",
       "        1.00228203],\n",
       "       [0.9986617 , 1.0729275 , 0.89186723, 0.8971852 , 1.08159337,\n",
       "        0.78105736, 0.92045049, 0.97002241, 0.91287224, 1.0025745 ,\n",
       "        1.1054947 , 1.06899402, 0.98847383, 0.99685189, 1.14613843,\n",
       "        1.0069557 ],\n",
       "       [0.82752095, 1.03257376, 1.12704376, 0.87028255, 0.83393492,\n",
       "        0.93322103, 0.85547622, 1.02175917, 0.82847716, 1.11401238,\n",
       "        0.95314541, 1.09333589, 1.0750644 , 1.14606288, 1.12141005,\n",
       "        1.19756883]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(summ_state[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_reward(summ_state[1:], summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
